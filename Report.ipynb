{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of implementation\n",
    "\n",
    "The solution is implemented in PyTorch with Python 3. It is a DDPG actor critic implementation to solve the Reacher environment from UnityEnvironment. \n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of learning algorithm\n",
    "\n",
    "The implementation contains an Actor Citic model where the actor is a fully connected linear model (_Actor_-class) with two hidden layers (128 units each). When setting the *use_batchn* parameter to _true_, the actor uses batch normalization on each layer. The hidden layers are connected\n",
    "via a ReLu activation function while the final output layer uses tanh to yield an action-space sized output. The input layer receives the current state and thus has 33 units. \n",
    "The critic model (_Critic_-class) similarly consists of two hidden layers with 128 units each. The main difference is the concatination with the action vector, the output size (1) and the use of batch normalization  on the first layer only. \n",
    "\n",
    "For learning, the agent uses the model in combination with Experience Replay which is initialized in the _Replay Buffer_ class. The _Update Every_ parameter controls after how many steps the model learns. \n",
    "Additionally, in the step function of the agent it is implemented that the agent learns after 20 time steps 10 times, as suggested by the previous lecture text. \n",
    "Also, a parameter Epsilon was added that is used to reduce the noise the longer the agent trains and is designed to decay over time\n",
    "\n",
    "The hyperparameters were chosen as follows:\n",
    "\n",
    "- Buffer size for the Replay Buffer: 100000\n",
    "- Batch size: 256\n",
    "- Discount factor GAMMA: 0.99\n",
    "- Parameter for soft update (TAU): 0.001\n",
    "- Learning rate actor: 0.001\n",
    "- Learning rate critic: 0.001\n",
    "- Weight decay: 0\n",
    "- Update parameter to determine after how many steps the model learns: 1\n",
    "- Epsilon: 1\n",
    "- Epsilon decay: 1e-6\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of results\n",
    "\n",
    "<img src=\"Plot of results_solution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of last 100 episodes\n",
    "\n",
    "<img src=\"Plot of last 100 episodes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodes needed\n",
    "\n",
    "The total number of episodes that was needed to solve the environment with an average score of +30 over 100 episodes was 266 in the submitted implementation.\n",
    "\n",
    " <img src=\"Reacher_Results.PNG\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "Developing the model to work in combination with Prioritized Experience Replay or Hindsight Experience Replay seems interesting as well as comparing a PPO approach to the DDPG one.\n",
    "Also, it would be interesting to see how the trained agent would perform in similar but different environments to see if (1) it can learn faster in a new environment than a completely new agent and (2) there are other parallels between different environments and problems.\n",
    "\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
