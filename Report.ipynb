{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of implementation\n",
    "\n",
    "The solution is implemented in PyTorch with Python 3. It is a DDPG actor critic implementation to solve the Reacher environment from UnityEnvironment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of learning algorithm\n",
    "\n",
    "The implementation contains an Actor Citic model where the actor is a fully connected linear model (_Actor_-class) with two hidden layers (128 units each). When setting the *use_batchn* parameter to _true_, the actor uses batch normalization on each layer. The hidden layers are connected"
    "via a ReLu activation function while the final output layer uses tanh to yield an action-space sized output. The input layer receives the current state and thus has 33 units. \n",
    "\n",
    "For learning, the agent uses the DQN in combination with Experience Replay which is initialized in the _Replay Buffer_ class. The _Update Every_ parameter controls after how many steps the model learns. \n",
    "The agent acts based on an epsilon-greedy policy whereas epsilon decreases over time to capitalize the learnt model parameters more and more over time. \n",
    "\n",
    "The hyperparameters were chosen as follows:\n",
    "\n",
    "- Hidden layer dropout probability: 0.1\n",
    "- Buffer size for the Replay Buffer: 100000\n",
    "- Batch size: 64\n",
    "- Discount factor GAMMA: 0.995\n",
    "- Parameter for soft update (TAU): 0.001\n",
    "- Learning rate: 0.0005\n",
    "- Update parameter to determine after how many steps the model learns: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of results\n",
    "\n",
    "<img src=\"Plot of results_solution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of last 100 episodes\n",
    "\n",
    "<img src=\"Plot of last 100 episodes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodes needed\n",
    "\n",
    "The total number of episodes that was needed to solve the environment with an average score of +13 over 100 episodes was 696 in the submitted implementation.\n",
    "\n",
    " <img src=\"Reacher_Results.PNG\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "As the current implementation 'only' is a DQN, future work could introduce concepts such as DDQN or Dueling DQN combined with Prioritized Experience Replay. Also, it would be interesting to see how the trained agent would perform in similar but different environments to see if (1) it can learn faster in a new environment than a completely new agent and (2) there are other parallels between different environments and problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
