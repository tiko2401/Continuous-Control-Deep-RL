{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of implementation\n",
    "\n",
    "The solution is implemented in PyTorch with Python 3. It is a DDPG actor critic implementation to solve the Reacher environment from UnityEnvironment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of learning algorithm\n",
    "\n",
    "The implementation contains a DQN-Agent (_Agent_-class) that uses a fully connected linear model (_QNetwork_-class) with three hidden layers (100, 200, 100 units). The input layer receives the current state and thus has 37 units. The output layer is connected to the action state and yields 4 units. The hidden layers are further connected through dropout layers. \n",
    "\n",
    "For learning, the agent uses the DQN in combination with Experience Replay which is initialized in the _Replay Buffer_ class. The _Update Every_ parameter controls after how many steps the model learns. \n",
    "The agent acts based on an epsilon-greedy policy whereas epsilon decreases over time to capitalize the learnt model parameters more and more over time. \n",
    "\n",
    "The hyperparameters were chosen as follows:\n",
    "\n",
    "- Hidden layer dropout probability: 0.1\n",
    "- Buffer size for the Replay Buffer: 100000\n",
    "- Batch size: 64\n",
    "- Discount factor GAMMA: 0.995\n",
    "- Parameter for soft update (TAU): 0.001\n",
    "- Learning rate: 0.0005\n",
    "- Update parameter to determine after how many steps the model learns: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of results\n",
    "\n",
    "<img src=\"Plot of results_solution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of last 100 episodes\n",
    "\n",
    "<img src=\"Plot of last 100 episodes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodes needed\n",
    "\n",
    "The total number of episodes that was needed to solve the environment with an average score of +13 over 100 episodes was 696 in the submitted implementation.\n",
    "\n",
    " <img src=\"Reacher_Results.PNG\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "As the current implementation 'only' is a DQN, future work could introduce concepts such as DDQN or Dueling DQN combined with Prioritized Experience Replay. Also, it would be interesting to see how the trained agent would perform in similar but different environments to see if (1) it can learn faster in a new environment than a completely new agent and (2) there are other parallels between different environments and problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
